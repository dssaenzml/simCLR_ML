{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml701-simCLR-RN50-V2-PL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dssaenzml/simCLR_ML/blob/main/ml701_simCLR_RN50_V2_PL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTtZvzLyFJ9C"
      },
      "source": [
        "# SimCLR with PL + Downstream\n",
        "\n",
        "List of full videos is here:    \n",
        "\n",
        "https://www.youtube.com/playlist?list=PLaMu-SDt_RB4k8VXiB3hOdsn0Y3GoXo1k\n",
        "\n",
        "Pretrained simCLR2: https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/pretrained?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-C_4NI9EElT"
      },
      "source": [
        "# ------- USE FOR TPU SUPPPORT ------\n",
        "# %%capture\n",
        "# ! pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl\n",
        "# torch.tensor([10.]*10000000000) # trick to gain RAM ( doesnt work anymore...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zna_bcK-dtvT"
      },
      "source": [
        "%%capture\n",
        "! pip install pytorch-lightning-bolts\n",
        "! pip install pytorch-lightning\n",
        "! pip install wandb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2hkbQR2TtP-"
      },
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pl_bolts.models.self_supervised import SimCLR\n",
        "from pl_bolts.callbacks.ssl_online import SSLOnlineEvaluator\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "from pl_bolts.models.self_supervised.simclr.transforms import (\n",
        "    SimCLREvalDataTransform, SimCLRTrainDataTransform)\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67u3AZxRlcym"
      },
      "source": [
        "# data\n",
        "dm = CIFAR10DataModule(num_workers=0)\n",
        "dm.train_transforms = SimCLRTrainDataTransform(32)\n",
        "dm.val_transforms = SimCLREvalDataTransform(32)\n",
        "\n",
        "\n",
        "# model\n",
        "model = SimCLR(max_epochs=5,num_samples=dm.num_samples, batch_size=dm.batch_size, dataset='cifar10', gpus=1)\n",
        "\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath='simclr_ckp',\n",
        "    monitor='val_loss',\n",
        "    filename='{epoch}-{train_loss:.2f}-{val_loss:.2f}'    \n",
        ")\n",
        "\n",
        "# fit\n",
        "trainer = pl.Trainer(max_epochs=1,progress_bar_refresh_rate=20,gpus=1,logger=wandb_logger,callbacks=[checkpoint_callback])\n",
        "trainer.fit(model, datamodule=dm)\n",
        "\n",
        "wandb_logger = WandbLogger(name='exp_1-epochs_170221',project='simCLR-ml701', id='2', log_model=True)\n",
        "wandb_logger.watch(model, log=\"all\", log_freq=50)\n",
        "\n",
        "#save checkpoint with weights\n",
        "checkpoint_file = \"resnet50-cifar10-embeddings.ckpt\"\n",
        "trainer.save_checkpoint(checkpoint_file)\n",
        "#save to W&B\n",
        "trainer.logger.experiment.log_artifact(checkpoint_file, type=\"model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX1UhEBKM8ac"
      },
      "source": [
        "#end run on W&B + sync results\r\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2qS69I8cfPR"
      },
      "source": [
        "# DOWNSTREAM TASK CLASSIFIER\r\n",
        "class MyClassifier(nn.Module):\r\n",
        "    def __init__(self, n_classes, freeze_base, embeddings_model_path, hidden_size=512):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        base_model = SimCLR.load_from_checkpoint(embeddings_model_path).model\r\n",
        "        \r\n",
        "        self.embeddings = base_model.embedding\r\n",
        "        \r\n",
        "        if freeze_base:\r\n",
        "            print(\"Freezing embeddings\")\r\n",
        "            for param in self.embeddings.parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "                \r\n",
        "        # Only linear projection on top of the embeddings should be enough\r\n",
        "        self.classifier = nn.Linear(in_features=base_model.projection[0].in_features, \r\n",
        "                      out_features = n_classes if n_classes &gt; 2 else 1)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, X, *args):\r\n",
        "        emb = self.embeddings(X)\r\n",
        "        return self.classifier(emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQQ-zj_mUdzK"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\r\n",
        "from argparse import Namespace\r\n",
        "\r\n",
        "class MyClassifierModule(pl.LightningModule):\r\n",
        "    def __init__(self, hparams):\r\n",
        "        super().__init__()\r\n",
        "        hparams = Namespace(**hparams) if isinstance(hparams, dict) else hparams\r\n",
        "        self.hparams = hparams\r\n",
        "        self.model = MyShittyClassifier(hparams.n_classes, hparams.freeze_base, \r\n",
        "                                      hparams.embeddings_path,\r\n",
        "                                      self.hparams.hidden_size)\r\n",
        "        self.loss = nn.CrossEntropyLoss()\r\n",
        "    \r\n",
        "    def total_steps(self):\r\n",
        "        return len(self.train_dataloader()) // self.hparams.epochs\r\n",
        "    \r\n",
        "    def preprocessing(seff):\r\n",
        "        return transforms.Compose([\r\n",
        "                transforms.ToTensor(),\r\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n",
        "        ])\r\n",
        "    \r\n",
        "    #TODO: look at custom data loading - check docs\r\n",
        "    def get_dataloader(self, split):\r\n",
        "        return DataLoader(CIFAR10DataModule(\".\", split=split, transform=self.preprocessing()),\r\n",
        "                          batch_size=self.hparams.batch_size, \r\n",
        "                          shuffle=split==\"train\",\r\n",
        "                          num_workers=cpu_count(),\r\n",
        "                         drop_last=False)\r\n",
        "    \r\n",
        "    def train_dataloader(self):\r\n",
        "        return self.get_dataloader(\"train\")\r\n",
        "    \r\n",
        "    def val_dataloader(self):\r\n",
        "        return self.get_dataloader(\"test\")\r\n",
        "    \r\n",
        "    def forward(self, X):\r\n",
        "        return self.model(X)\r\n",
        "    \r\n",
        "    def step(self, batch, step_name = \"train\"):\r\n",
        "        X, y = batch\r\n",
        "        y_out = self.forward(X)\r\n",
        "        loss = self.loss(y_out, y)\r\n",
        "        loss_key = f\"{step_name}_loss\"\r\n",
        "        tensorboard_logs = {loss_key: loss}\r\n",
        "\r\n",
        "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\r\n",
        "                        \"progress_bar\": {loss_key: loss}}\r\n",
        "    \r\n",
        "    def training_step(self, batch, batch_idx):\r\n",
        "        return self.step(batch, \"train\")\r\n",
        "    \r\n",
        "    def validation_step(self, batch, batch_idx):\r\n",
        "        return self.step(batch, \"val\")\r\n",
        "    \r\n",
        "    def test_step(self, batch, batch_idx):\r\n",
        "        return self.step(batch, \"test\")\r\n",
        "    \r\n",
        "    def validation_end(self, outputs):\r\n",
        "        if len(outputs) == 0:\r\n",
        "            return {\"val_loss\": torch.tensor(0)}\r\n",
        "        else:\r\n",
        "            loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\r\n",
        "            return {\"val_loss\": loss, \"log\": {\"val_loss\": loss}}\r\n",
        "\r\n",
        "  #TODO: explore other schedulers and optimizers\r\n",
        "  \r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = RMSprop(self.model.parameters(), lr=self.hparams.lr)\r\n",
        "        schedulers = [\r\n",
        "            CosineAnnealingLR(optimizer, self.hparams.epochs)\r\n",
        "        ] if self.hparams.epochs &gt; 1 else []\r\n",
        "        return [optimizer], schedulers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhvrtSfg5yYt"
      },
      "source": [
        "\r\n",
        "hparams_cls = Namespace(\r\n",
        "    lr=1e-3,\r\n",
        "    epochs=5,\r\n",
        "    batch_size=160,\r\n",
        "    n_classes=10,\r\n",
        "    freeze_base=True,\r\n",
        "    embeddings_path=\"resnet50-cifar10-embeddings.ckpt\",\r\n",
        "    hidden_size=512\r\n",
        ")\r\n",
        "module = MyClassifierModule(hparams_cls)\r\n",
        "\r\n",
        "wandb_logger = WandbLogger(name='classifier-exp-1',project='simCLR-ml701', id='d1', log_model=True)\r\n",
        "wandb_logger.watch(module, log=\"all\", log_freq=50)\r\n",
        "\r\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=hparams_cls.epochs, logger=wandb_logger)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy-PlauM8xfv"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "def evaluate(data_loader, module):\r\n",
        "    with torch.no_grad():\r\n",
        "        progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\r\n",
        "        module.eval().cuda()\r\n",
        "        true_y, pred_y = [], []\r\n",
        "        for i, batch_ in enumerate(data_loader):\r\n",
        "            X, y = batch_\r\n",
        "            print(progress[i % len(progress)], end=\"\\r\")\r\n",
        "            y_pred = torch.argmax(module(X.cuda()), dim=1)\r\n",
        "            true_y.extend(y.cpu())\r\n",
        "            pred_y.extend(y_pred.cpu())\r\n",
        "        print(classification_report(true_y, pred_y, digits=3))\r\n",
        "        return true_y, pred_y\r\n",
        "        \r\n",
        "_ = evaluate(module.val_dataloader(), module)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}